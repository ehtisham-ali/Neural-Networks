{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    train = np.copy(X_trainval)\n",
    "    test  = np.copy(X_test)\n",
    "    for row in np.transpose(train):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row= row / deviation\n",
    "        row -= np.mean(row)\n",
    "        \n",
    "    for row in np.transpose(test):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row = row / deviation\n",
    "        row -= np.mean(row)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print(np.shape(X_trainval),np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    \"\"\"\n",
    "    #TODO: Implement\n",
    "    #The first ``n_samples % n_splits`` folds have size\n",
    "    # ``n_samples // n_splits + 1``, other folds have size\n",
    "    # ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
    "    n_sample = config.num_train\n",
    "    fold_sizes = np.full(K, n_sample // K, dtype=np.int)\n",
    "    fold_sizes[:n_sample % K] += 1\n",
    "    curr_fold_size = fold_sizes[i]\n",
    "    \n",
    "    X_train_size = curr_fold_size \n",
    "    X_val_size =  curr_fold_size - X_train_size\n",
    "        \n",
    "    s_index = sum(fold_sizes[:i-1])\n",
    "    X_val = X_trainval[int(s_index+X_train_size): int(s_index+X_train_size+X_val_size)]\n",
    "    Y_val = Y_trainval[int(s_index+X_train_size): int(s_index+X_train_size+X_val_size)]\n",
    "    \n",
    "    X_train = X_trainval[int(s_index): int(s_index+X_train_size)]\n",
    "    Y_train = Y_trainval[int(s_index): int(s_index+X_train_size)]\n",
    "    \n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "#X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.\n",
    "            # w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "            w = tf.Variable(tf.random_normal( shape =[784 ,10], stddev = 0.01 ), name = \"weights\")\n",
    "            b = tf .Variable( tf.zeros([1 ,10]), name = \"bias\")\n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            output = tf.matmul(X , w ) + b \n",
    "                #loss function without regularizer and compute mean over examples in the batch\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = output, labels=Y_gt))\n",
    "                # Loss function using L2 Regularization\n",
    "            regularizer = tf.nn.l2_loss(w)\n",
    "            self._loss = tf.reduce_mean(loss + config.lambd * regularizer)\n",
    "            \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "            \n",
    "            #TODO: Compute the accuracy\n",
    "            predictions = tf.nn.softmax(output)\n",
    "            c_prediction=tf.equal(tf.argmax(predictions,1),tf.argmax(Y_gt,1))\n",
    "            self._num_acc =tf.reduce_mean(tf.cast(c_prediction,tf.float32))\n",
    "            \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "    for i in range ( int(len(X_test)/config.batch_size)):\n",
    "        _ , cost , acc = sess.run([model.loss(),model.num_acc()], \n",
    "                                  feed_dict ={ X: X_test , Y_gt: Y_test, config:config})\n",
    "        total_cost = total_cost + cost\n",
    "        accs = accs + acc\n",
    "    \n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement\n",
    "        shuffle_train_data(X_train,Y_train)\n",
    "        for _ in range (int(len(X_train)/config.batch_size)):\n",
    "            _,cost_trains, acc_trains = sess.run([model.loss, model.num_acc], feed_dict = {X: X_train, Y_gt: Y_train})\n",
    "           \n",
    "        cost_trains.append(cost_train)\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" %  cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "        #Validation\n",
    "        cost_vals,acc_vals = testing(model, X_val, Y_val, config)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        \n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected dimension in the range [-1, 1), but got 1\n\t [[Node: 100_3/ArgMax_1 = ArgMax[T=DT_INT64, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_1_0_1, 100_3/ArgMax/dimension)]]\n\nCaused by op '100_3/ArgMax_1', defined at:\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-cd4d70adec30>\", line 21, in <module>\n    model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n  File \"<ipython-input-10-1ab54429bc4f>\", line 42, in __init__\n    c_prediction=tf.equal(tf.argmax(predictions,1),tf.argmax(Y_gt,1))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 88, in argmax\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 783, in arg_max\n    name=name)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\n\t [[Node: 100_3/ArgMax_1 = ArgMax[T=DT_INT64, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_1_0_1, 100_3/ArgMax/dimension)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[Node: 100_3/ArgMax_1 = ArgMax[T=DT_INT64, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_1_0_1, 100_3/ArgMax/dimension)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd4d70adec30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcost_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mval_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-82aa3de9c14f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, X_val, Y_train, Y_val, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mshuffle_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_gt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcost_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[Node: 100_3/ArgMax_1 = ArgMax[T=DT_INT64, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_1_0_1, 100_3/ArgMax/dimension)]]\n\nCaused by op '100_3/ArgMax_1', defined at:\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-cd4d70adec30>\", line 21, in <module>\n    model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n  File \"<ipython-input-10-1ab54429bc4f>\", line 42, in __init__\n    c_prediction=tf.equal(tf.argmax(predictions,1),tf.argmax(Y_gt,1))\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 88, in argmax\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 783, in arg_max\n    name=name)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/ehtisham/anaconda3/envs/nnia/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\n\t [[Node: 100_3/ArgMax_1 = ArgMax[T=DT_INT64, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_1_0_1, 100_3/ArgMax/dimension)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-fd25e93c524b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-fd25e93c524b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    config.lambd =  #TODO: Choose the best lambda\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "config.lambd =  #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import math\n",
    "from scipy.special import expit as sigmoid1\n",
    "from scipy.stats import logistic\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        w1 = np.random.uniform(low = 0, high = 11, size= self.n_features*self.n_hidden)\n",
    "        self.w1 = w1.reshape(self.n_features, self.n_hidden)\n",
    "        w2 = np.random.uniform(low = 0, high = 11, size= self.n_hidden*self.n_output)\n",
    "        self.w2 = w2.reshape(self.n_hidden, self.n_output)\n",
    "        \n",
    "        self.cost_iter = np.zeros([epochs * batchsize])\n",
    "        #bias mai dimension hidden aur output ka hoga \n",
    "        self.b1 = np.random.uniform(low = 0, high = 1, size = self.n_hidden)\n",
    "        self.b2 = np.random.uniform(low = 0, high = 1, size = self.n_output)\n",
    "\n",
    "        \n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        #res = np.zeros([k])\n",
    "        #res[y] = 1\n",
    "        res = np.zeros([y.shape[0],k])\n",
    "        for i in range(y.shape[0]):\n",
    "            res[i,y[i]] = 1 \n",
    "        return res\n",
    "        \n",
    "        \n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        return 1/(1 + np.exp(-z))\n",
    "        #TODO Implement\n",
    "\n",
    "        \n",
    "    def grad_softmax(self, x):\n",
    "        e_x = np.exp(np.subtract(x, np.max(x)))\n",
    "        # print(e_x)\n",
    "        n = (e_x / np.sum(e_x, axis=0)) + np.square((e_x)/(np.sum(e_x,axis=0)))\n",
    "        return n\n",
    "        \n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        self.temp_sig = self.sigmoid(z)\n",
    "        return self.temp_sig * (1 - self.temp_sig)\n",
    "\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "        numerator = np.exp(z - np.amax(z))\n",
    "        denominator = np.sum(numerator)\n",
    "        softmax_output = np.exp(z - np.amax(z))/denominator \n",
    "        return softmax_output\n",
    "\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2 = np.dot(X, self.w1) + self.b1        \n",
    "        \n",
    "        a2 = self.sigmoid(z2)        \n",
    "        \n",
    "        z3 = np.dot(a2, self.w2) + self.b2      \n",
    "        \n",
    "        a3 = self.softmax(z3)       \n",
    "        \n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        \n",
    "        x = lambd * np.sum(np.square(self.w1))\n",
    "        y = lambd * np.sum(np.square(self.w2))\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement total loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement        \n",
    "        m = y_enc.shape[0]\n",
    "        \n",
    "        y = y_enc.argmax(axis=1)\n",
    "        \n",
    "        log_likelihood = -np.log(output[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def hot_encode_vector(self, y):\n",
    "        \n",
    "        rows = y.shape[0]\n",
    "        \n",
    "        b = np.zeros((rows, 10))\n",
    "        b[np.arange(rows), y] = 1\n",
    "        \n",
    "        return b\n",
    "    \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, z3, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        dz3   = np.subtract(a3, y_enc)\n",
    "        \n",
    "        \n",
    "        h2_ = self.grad_softmax(z3)\n",
    "        delta2 = np.multiply(h2_, np.subtract(y_enc, a3))\n",
    "        \n",
    "        h1_ = self.sigmoid_gradient(z2)\n",
    "        delta1 = np.multiply(np.matmul(self.w2, np.transpose(delta2)), np.transpose(h1_))\n",
    "        \n",
    "        grad1 = np.matmul(np.transpose(X), np.transpose(delta1))\n",
    "        grad2 = np.matmul(np.transpose(a2), delta2)\n",
    "        \n",
    "        # regularize\n",
    "        regul21 = 2 * self.l2 * self.w1\n",
    "        regul22 = 2 * self.l2 * self.w2\n",
    "        \n",
    "        res1 = np.add (grad1, regul21)\n",
    "        res2 = np.add (grad2, regul22)\n",
    "        \n",
    "        grad4 = (1 / self.batchsize) * np.sum(dz3, axis=1, keepdims=True)\n",
    "        grad3 = (1 / self.batchsize) * np.sum(delta1, axis=1, keepdims=True)\n",
    "        \n",
    "        #res1 is the Gradient of the weight matrix w1 with L2 reg\n",
    "        #res2 is the Gradient of the weight matrix w2 with L2 reg\n",
    "        return res1, res2, grad3, grad4 \n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2 = np.dot(X, self.w1)         \n",
    "        \n",
    "        a2 = self.sigmoid(z2)        \n",
    "        \n",
    "        z3 = np.dot(a2, self.w2)       \n",
    "        \n",
    "        a3 = self.softmax(z3)       \n",
    "                \n",
    "        y_pred = a3 > 0.5\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            batch_size = math.floor(X_train.shape[0]/self.batchsize)\n",
    "            acc_grad1 = np.zeros(self.w1.shape)\n",
    "            acc_grad2 = np.zeros(self.w2.shape)\n",
    "            Y_hot = self.encode_labels(Y_train, 10)\n",
    "            \n",
    "            for i in range(self.batchsize):\n",
    "                X_batch = X_train[i * batch_size:(i+1) * batch_size - 1]\n",
    "                Y_batch = Y_hot[i*batch_size:(i+1)*batch_size-1]\n",
    "                \n",
    "                # feedforward\n",
    "                z2, a2, z3, a3 = self.forward(X_batch)\n",
    "                grad1, grad2, grad3, grad4 = self.compute_gradient(X_batch,a2, a3 ,z2,z3, Y_batch)\n",
    "                acc_grad1 = np.add(acc_grad1, grad1)\n",
    "                acc_grad2 = np.add(acc_grad2, grad2)\n",
    "                \n",
    "                _,_,_, output1 = self.forward(X_batch)\n",
    "                self.cost_iter[(e * self.batchsize + i)] = self.loss(Y_batch, output1)\n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (e+1, self.epochs))\n",
    "\n",
    "\n",
    "                # feedforward and loss computation\n",
    "                _,_,_, output= self.forward(X_train)\n",
    "\n",
    "                #self.cost_.append(cost)\n",
    "                                \n",
    "                self.cost_.append(self.loss(Y_hot, output))\n",
    "                self.w1 = np.subtract(self.w1, np.multiply(self.lr, acc_grad1))\n",
    "                self.w2 = np.subtract(self.w2, np.multiply(self.lr, acc_grad2))\n",
    "\n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features= X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=500, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/500\n",
      "\n",
      "Epoch: 2/500\n",
      "\n",
      "Epoch: 3/500\n",
      "\n",
      "Epoch: 4/500\n",
      "\n",
      "Epoch: 5/500\n",
      "\n",
      "Epoch: 6/500\n",
      "\n",
      "Epoch: 7/500\n",
      "\n",
      "Epoch: 8/500\n",
      "\n",
      "Epoch: 9/500\n",
      "\n",
      "Epoch: 10/500\n",
      "\n",
      "Epoch: 11/500\n",
      "\n",
      "Epoch: 12/500\n",
      "\n",
      "Epoch: 13/500\n",
      "\n",
      "Epoch: 14/500\n",
      "\n",
      "Epoch: 15/500\n",
      "\n",
      "Epoch: 16/500\n",
      "\n",
      "Epoch: 17/500\n",
      "\n",
      "Epoch: 18/500\n",
      "\n",
      "Epoch: 19/500\n",
      "\n",
      "Epoch: 20/500\n",
      "\n",
      "Epoch: 21/500\n",
      "\n",
      "Epoch: 22/500\n",
      "\n",
      "Epoch: 23/500\n",
      "\n",
      "Epoch: 24/500\n",
      "\n",
      "Epoch: 25/500\n",
      "\n",
      "Epoch: 26/500\n",
      "\n",
      "Epoch: 27/500\n",
      "\n",
      "Epoch: 28/500\n",
      "\n",
      "Epoch: 29/500\n",
      "\n",
      "Epoch: 30/500\n",
      "\n",
      "Epoch: 31/500\n",
      "\n",
      "Epoch: 32/500\n",
      "\n",
      "Epoch: 33/500\n",
      "\n",
      "Epoch: 34/500\n",
      "\n",
      "Epoch: 35/500\n",
      "\n",
      "Epoch: 36/500\n",
      "\n",
      "Epoch: 37/500\n",
      "\n",
      "Epoch: 38/500\n",
      "\n",
      "Epoch: 39/500\n",
      "\n",
      "Epoch: 40/500\n",
      "\n",
      "Epoch: 41/500\n",
      "\n",
      "Epoch: 42/500\n",
      "\n",
      "Epoch: 43/500\n",
      "\n",
      "Epoch: 44/500\n",
      "\n",
      "Epoch: 45/500\n",
      "\n",
      "Epoch: 46/500\n",
      "\n",
      "Epoch: 47/500\n",
      "\n",
      "Epoch: 48/500\n",
      "\n",
      "Epoch: 49/500\n",
      "\n",
      "Epoch: 50/500\n",
      "\n",
      "Epoch: 51/500\n",
      "\n",
      "Epoch: 52/500\n",
      "\n",
      "Epoch: 53/500\n",
      "\n",
      "Epoch: 54/500\n",
      "\n",
      "Epoch: 55/500\n",
      "\n",
      "Epoch: 56/500\n",
      "\n",
      "Epoch: 57/500\n",
      "\n",
      "Epoch: 58/500\n",
      "\n",
      "Epoch: 59/500\n",
      "\n",
      "Epoch: 60/500\n",
      "\n",
      "Epoch: 61/500\n",
      "\n",
      "Epoch: 62/500\n",
      "\n",
      "Epoch: 63/500\n",
      "\n",
      "Epoch: 64/500\n",
      "\n",
      "Epoch: 65/500\n",
      "\n",
      "Epoch: 66/500\n",
      "\n",
      "Epoch: 67/500\n",
      "\n",
      "Epoch: 68/500\n",
      "\n",
      "Epoch: 69/500\n",
      "\n",
      "Epoch: 70/500\n",
      "\n",
      "Epoch: 71/500\n",
      "\n",
      "Epoch: 72/500\n",
      "\n",
      "Epoch: 73/500\n",
      "\n",
      "Epoch: 74/500\n",
      "\n",
      "Epoch: 75/500\n",
      "\n",
      "Epoch: 76/500\n",
      "\n",
      "Epoch: 77/500\n",
      "\n",
      "Epoch: 78/500\n",
      "\n",
      "Epoch: 79/500\n",
      "\n",
      "Epoch: 80/500\n",
      "\n",
      "Epoch: 81/500\n",
      "\n",
      "Epoch: 82/500\n",
      "\n",
      "Epoch: 83/500\n",
      "\n",
      "Epoch: 84/500\n",
      "\n",
      "Epoch: 85/500\n",
      "\n",
      "Epoch: 86/500\n",
      "\n",
      "Epoch: 87/500\n",
      "\n",
      "Epoch: 88/500\n",
      "\n",
      "Epoch: 89/500\n",
      "\n",
      "Epoch: 90/500\n",
      "\n",
      "Epoch: 91/500\n",
      "\n",
      "Epoch: 92/500\n",
      "\n",
      "Epoch: 93/500\n",
      "\n",
      "Epoch: 94/500\n",
      "\n",
      "Epoch: 95/500\n",
      "\n",
      "Epoch: 96/500\n",
      "\n",
      "Epoch: 97/500\n",
      "\n",
      "Epoch: 98/500\n",
      "\n",
      "Epoch: 99/500\n",
      "\n",
      "Epoch: 100/500\n",
      "\n",
      "Epoch: 101/500\n",
      "\n",
      "Epoch: 102/500\n",
      "\n",
      "Epoch: 103/500\n",
      "\n",
      "Epoch: 104/500\n",
      "\n",
      "Epoch: 105/500\n",
      "\n",
      "Epoch: 106/500\n",
      "\n",
      "Epoch: 107/500\n",
      "\n",
      "Epoch: 108/500\n",
      "\n",
      "Epoch: 109/500\n",
      "\n",
      "Epoch: 110/500\n",
      "\n",
      "Epoch: 111/500\n",
      "\n",
      "Epoch: 112/500\n",
      "\n",
      "Epoch: 113/500\n",
      "\n",
      "Epoch: 114/500\n",
      "\n",
      "Epoch: 115/500\n",
      "\n",
      "Epoch: 116/500\n",
      "\n",
      "Epoch: 117/500\n",
      "\n",
      "Epoch: 118/500\n",
      "\n",
      "Epoch: 119/500\n",
      "\n",
      "Epoch: 120/500\n",
      "\n",
      "Epoch: 121/500\n",
      "\n",
      "Epoch: 122/500\n",
      "\n",
      "Epoch: 123/500\n",
      "\n",
      "Epoch: 124/500\n",
      "\n",
      "Epoch: 125/500\n",
      "\n",
      "Epoch: 126/500\n",
      "\n",
      "Epoch: 127/500\n",
      "\n",
      "Epoch: 128/500\n",
      "\n",
      "Epoch: 129/500\n",
      "\n",
      "Epoch: 130/500\n",
      "\n",
      "Epoch: 131/500\n",
      "\n",
      "Epoch: 132/500\n",
      "\n",
      "Epoch: 133/500\n",
      "\n",
      "Epoch: 134/500\n",
      "\n",
      "Epoch: 135/500\n",
      "\n",
      "Epoch: 136/500\n",
      "\n",
      "Epoch: 137/500\n",
      "\n",
      "Epoch: 138/500\n",
      "\n",
      "Epoch: 139/500\n",
      "\n",
      "Epoch: 140/500\n",
      "\n",
      "Epoch: 141/500\n",
      "\n",
      "Epoch: 142/500\n",
      "\n",
      "Epoch: 143/500\n",
      "\n",
      "Epoch: 144/500\n",
      "\n",
      "Epoch: 145/500\n",
      "\n",
      "Epoch: 146/500\n",
      "\n",
      "Epoch: 147/500\n",
      "\n",
      "Epoch: 148/500\n",
      "\n",
      "Epoch: 149/500\n",
      "\n",
      "Epoch: 150/500\n",
      "\n",
      "Epoch: 151/500\n",
      "\n",
      "Epoch: 152/500\n",
      "\n",
      "Epoch: 153/500\n",
      "\n",
      "Epoch: 154/500\n",
      "\n",
      "Epoch: 155/500\n",
      "\n",
      "Epoch: 156/500\n",
      "\n",
      "Epoch: 157/500\n",
      "\n",
      "Epoch: 158/500\n",
      "\n",
      "Epoch: 159/500\n",
      "\n",
      "Epoch: 160/500\n",
      "\n",
      "Epoch: 161/500\n",
      "\n",
      "Epoch: 162/500\n",
      "\n",
      "Epoch: 163/500\n",
      "\n",
      "Epoch: 164/500\n",
      "\n",
      "Epoch: 165/500\n",
      "\n",
      "Epoch: 166/500\n",
      "\n",
      "Epoch: 167/500\n",
      "\n",
      "Epoch: 168/500\n",
      "\n",
      "Epoch: 169/500\n",
      "\n",
      "Epoch: 170/500\n",
      "\n",
      "Epoch: 171/500\n",
      "\n",
      "Epoch: 172/500\n",
      "\n",
      "Epoch: 173/500\n",
      "\n",
      "Epoch: 174/500\n",
      "\n",
      "Epoch: 175/500\n",
      "\n",
      "Epoch: 176/500\n",
      "\n",
      "Epoch: 177/500\n",
      "\n",
      "Epoch: 178/500\n",
      "\n",
      "Epoch: 179/500\n",
      "\n",
      "Epoch: 180/500\n",
      "\n",
      "Epoch: 181/500\n",
      "\n",
      "Epoch: 182/500\n",
      "\n",
      "Epoch: 183/500\n",
      "\n",
      "Epoch: 184/500\n",
      "\n",
      "Epoch: 185/500\n",
      "\n",
      "Epoch: 186/500\n",
      "\n",
      "Epoch: 187/500\n",
      "\n",
      "Epoch: 188/500\n",
      "\n",
      "Epoch: 189/500\n",
      "\n",
      "Epoch: 190/500\n",
      "\n",
      "Epoch: 191/500\n",
      "\n",
      "Epoch: 192/500\n",
      "\n",
      "Epoch: 193/500\n",
      "\n",
      "Epoch: 194/500\n",
      "\n",
      "Epoch: 195/500\n",
      "\n",
      "Epoch: 196/500\n",
      "\n",
      "Epoch: 197/500\n",
      "\n",
      "Epoch: 198/500\n",
      "\n",
      "Epoch: 199/500\n",
      "\n",
      "Epoch: 200/500\n",
      "\n",
      "Epoch: 201/500\n",
      "\n",
      "Epoch: 202/500\n",
      "\n",
      "Epoch: 203/500\n",
      "\n",
      "Epoch: 204/500\n",
      "\n",
      "Epoch: 205/500\n",
      "\n",
      "Epoch: 206/500\n",
      "\n",
      "Epoch: 207/500\n",
      "\n",
      "Epoch: 208/500\n",
      "\n",
      "Epoch: 209/500\n",
      "\n",
      "Epoch: 210/500\n",
      "\n",
      "Epoch: 211/500\n",
      "\n",
      "Epoch: 212/500\n",
      "\n",
      "Epoch: 213/500\n",
      "\n",
      "Epoch: 214/500\n",
      "\n",
      "Epoch: 215/500\n",
      "\n",
      "Epoch: 216/500\n",
      "\n",
      "Epoch: 217/500\n",
      "\n",
      "Epoch: 218/500\n",
      "\n",
      "Epoch: 219/500\n",
      "\n",
      "Epoch: 220/500\n",
      "\n",
      "Epoch: 221/500\n",
      "\n",
      "Epoch: 222/500\n",
      "\n",
      "Epoch: 223/500\n",
      "\n",
      "Epoch: 224/500\n",
      "\n",
      "Epoch: 225/500\n",
      "\n",
      "Epoch: 226/500\n",
      "\n",
      "Epoch: 227/500\n",
      "\n",
      "Epoch: 228/500\n",
      "\n",
      "Epoch: 229/500\n",
      "\n",
      "Epoch: 230/500\n",
      "\n",
      "Epoch: 231/500\n",
      "\n",
      "Epoch: 232/500\n",
      "\n",
      "Epoch: 233/500\n",
      "\n",
      "Epoch: 234/500\n",
      "\n",
      "Epoch: 235/500\n",
      "\n",
      "Epoch: 236/500\n",
      "\n",
      "Epoch: 237/500\n",
      "\n",
      "Epoch: 238/500\n",
      "\n",
      "Epoch: 239/500\n",
      "\n",
      "Epoch: 240/500\n",
      "\n",
      "Epoch: 241/500\n",
      "\n",
      "Epoch: 242/500\n",
      "\n",
      "Epoch: 243/500\n",
      "\n",
      "Epoch: 244/500\n",
      "\n",
      "Epoch: 245/500\n",
      "\n",
      "Epoch: 246/500\n",
      "\n",
      "Epoch: 247/500\n",
      "\n",
      "Epoch: 248/500\n",
      "\n",
      "Epoch: 249/500\n",
      "\n",
      "Epoch: 250/500\n",
      "\n",
      "Epoch: 251/500\n",
      "\n",
      "Epoch: 252/500\n",
      "\n",
      "Epoch: 253/500\n",
      "\n",
      "Epoch: 254/500\n",
      "\n",
      "Epoch: 255/500\n",
      "\n",
      "Epoch: 256/500\n",
      "\n",
      "Epoch: 257/500\n",
      "\n",
      "Epoch: 258/500\n",
      "\n",
      "Epoch: 259/500\n",
      "\n",
      "Epoch: 260/500\n",
      "\n",
      "Epoch: 261/500\n",
      "\n",
      "Epoch: 262/500\n",
      "\n",
      "Epoch: 263/500\n",
      "\n",
      "Epoch: 264/500\n",
      "\n",
      "Epoch: 265/500\n",
      "\n",
      "Epoch: 266/500\n",
      "\n",
      "Epoch: 267/500\n",
      "\n",
      "Epoch: 268/500\n",
      "\n",
      "Epoch: 269/500\n",
      "\n",
      "Epoch: 270/500\n",
      "\n",
      "Epoch: 271/500\n",
      "\n",
      "Epoch: 272/500\n",
      "\n",
      "Epoch: 273/500\n",
      "\n",
      "Epoch: 274/500\n",
      "\n",
      "Epoch: 275/500\n",
      "\n",
      "Epoch: 276/500\n",
      "\n",
      "Epoch: 277/500\n",
      "\n",
      "Epoch: 278/500\n",
      "\n",
      "Epoch: 279/500\n",
      "\n",
      "Epoch: 280/500\n",
      "\n",
      "Epoch: 281/500\n",
      "\n",
      "Epoch: 282/500\n",
      "\n",
      "Epoch: 283/500\n",
      "\n",
      "Epoch: 284/500\n",
      "\n",
      "Epoch: 285/500\n",
      "\n",
      "Epoch: 286/500\n",
      "\n",
      "Epoch: 287/500\n",
      "\n",
      "Epoch: 288/500\n",
      "\n",
      "Epoch: 289/500\n",
      "\n",
      "Epoch: 290/500\n",
      "\n",
      "Epoch: 291/500\n",
      "\n",
      "Epoch: 292/500\n",
      "\n",
      "Epoch: 293/500\n",
      "\n",
      "Epoch: 294/500\n",
      "\n",
      "Epoch: 295/500\n",
      "\n",
      "Epoch: 296/500\n",
      "\n",
      "Epoch: 297/500\n",
      "\n",
      "Epoch: 298/500\n",
      "\n",
      "Epoch: 299/500\n",
      "\n",
      "Epoch: 300/500\n",
      "\n",
      "Epoch: 301/500\n",
      "\n",
      "Epoch: 302/500\n",
      "\n",
      "Epoch: 303/500\n",
      "\n",
      "Epoch: 304/500\n",
      "\n",
      "Epoch: 305/500\n",
      "\n",
      "Epoch: 306/500\n",
      "\n",
      "Epoch: 307/500\n",
      "\n",
      "Epoch: 308/500\n",
      "\n",
      "Epoch: 309/500\n",
      "\n",
      "Epoch: 310/500\n",
      "\n",
      "Epoch: 311/500\n",
      "\n",
      "Epoch: 312/500\n",
      "\n",
      "Epoch: 313/500\n",
      "\n",
      "Epoch: 314/500\n",
      "\n",
      "Epoch: 315/500\n",
      "\n",
      "Epoch: 316/500\n",
      "\n",
      "Epoch: 317/500\n",
      "\n",
      "Epoch: 318/500\n",
      "\n",
      "Epoch: 319/500\n",
      "\n",
      "Epoch: 320/500\n",
      "\n",
      "Epoch: 321/500\n",
      "\n",
      "Epoch: 322/500\n",
      "\n",
      "Epoch: 323/500\n",
      "\n",
      "Epoch: 324/500\n",
      "\n",
      "Epoch: 325/500\n",
      "\n",
      "Epoch: 326/500\n",
      "\n",
      "Epoch: 327/500\n",
      "\n",
      "Epoch: 328/500\n",
      "\n",
      "Epoch: 329/500\n",
      "\n",
      "Epoch: 330/500\n",
      "\n",
      "Epoch: 331/500\n",
      "\n",
      "Epoch: 332/500\n",
      "\n",
      "Epoch: 333/500\n",
      "\n",
      "Epoch: 334/500\n",
      "\n",
      "Epoch: 335/500\n",
      "\n",
      "Epoch: 336/500\n",
      "\n",
      "Epoch: 337/500\n",
      "\n",
      "Epoch: 338/500\n",
      "\n",
      "Epoch: 339/500\n",
      "\n",
      "Epoch: 340/500\n",
      "\n",
      "Epoch: 341/500\n",
      "\n",
      "Epoch: 342/500\n",
      "\n",
      "Epoch: 343/500\n",
      "\n",
      "Epoch: 344/500\n",
      "\n",
      "Epoch: 345/500\n",
      "\n",
      "Epoch: 346/500\n",
      "\n",
      "Epoch: 347/500\n",
      "\n",
      "Epoch: 348/500\n",
      "\n",
      "Epoch: 349/500\n",
      "\n",
      "Epoch: 350/500\n",
      "\n",
      "Epoch: 351/500\n",
      "\n",
      "Epoch: 352/500\n",
      "\n",
      "Epoch: 353/500\n",
      "\n",
      "Epoch: 354/500\n",
      "\n",
      "Epoch: 355/500\n",
      "\n",
      "Epoch: 356/500\n",
      "\n",
      "Epoch: 357/500\n",
      "\n",
      "Epoch: 358/500\n",
      "\n",
      "Epoch: 359/500\n",
      "\n",
      "Epoch: 360/500\n",
      "\n",
      "Epoch: 361/500\n",
      "\n",
      "Epoch: 362/500\n",
      "\n",
      "Epoch: 363/500\n",
      "\n",
      "Epoch: 364/500\n",
      "\n",
      "Epoch: 365/500\n",
      "\n",
      "Epoch: 366/500\n",
      "\n",
      "Epoch: 367/500\n",
      "\n",
      "Epoch: 368/500\n",
      "\n",
      "Epoch: 369/500\n",
      "\n",
      "Epoch: 370/500\n",
      "\n",
      "Epoch: 371/500\n",
      "\n",
      "Epoch: 372/500\n",
      "\n",
      "Epoch: 373/500\n",
      "\n",
      "Epoch: 374/500\n",
      "\n",
      "Epoch: 375/500\n",
      "\n",
      "Epoch: 376/500\n",
      "\n",
      "Epoch: 377/500\n",
      "\n",
      "Epoch: 378/500\n",
      "\n",
      "Epoch: 379/500\n",
      "\n",
      "Epoch: 380/500\n",
      "\n",
      "Epoch: 381/500\n",
      "\n",
      "Epoch: 382/500\n",
      "\n",
      "Epoch: 383/500\n",
      "\n",
      "Epoch: 384/500\n",
      "\n",
      "Epoch: 385/500\n",
      "\n",
      "Epoch: 386/500\n",
      "\n",
      "Epoch: 387/500\n",
      "\n",
      "Epoch: 388/500\n",
      "\n",
      "Epoch: 389/500\n",
      "\n",
      "Epoch: 390/500\n",
      "\n",
      "Epoch: 391/500\n",
      "\n",
      "Epoch: 392/500\n",
      "\n",
      "Epoch: 393/500\n",
      "\n",
      "Epoch: 394/500\n",
      "\n",
      "Epoch: 395/500\n",
      "\n",
      "Epoch: 396/500\n",
      "\n",
      "Epoch: 397/500\n",
      "\n",
      "Epoch: 398/500\n",
      "\n",
      "Epoch: 399/500\n",
      "\n",
      "Epoch: 400/500\n",
      "\n",
      "Epoch: 401/500\n",
      "\n",
      "Epoch: 402/500\n",
      "\n",
      "Epoch: 403/500\n",
      "\n",
      "Epoch: 404/500\n",
      "\n",
      "Epoch: 405/500\n",
      "\n",
      "Epoch: 406/500\n",
      "\n",
      "Epoch: 407/500\n",
      "\n",
      "Epoch: 408/500\n",
      "\n",
      "Epoch: 409/500\n",
      "\n",
      "Epoch: 410/500\n",
      "\n",
      "Epoch: 411/500\n",
      "\n",
      "Epoch: 412/500\n",
      "\n",
      "Epoch: 413/500\n",
      "\n",
      "Epoch: 414/500\n",
      "\n",
      "Epoch: 415/500\n",
      "\n",
      "Epoch: 416/500\n",
      "\n",
      "Epoch: 417/500\n",
      "\n",
      "Epoch: 418/500\n",
      "\n",
      "Epoch: 419/500\n",
      "\n",
      "Epoch: 420/500\n",
      "\n",
      "Epoch: 421/500\n",
      "\n",
      "Epoch: 422/500\n",
      "\n",
      "Epoch: 423/500\n",
      "\n",
      "Epoch: 424/500\n",
      "\n",
      "Epoch: 425/500\n",
      "\n",
      "Epoch: 426/500\n",
      "\n",
      "Epoch: 427/500\n",
      "\n",
      "Epoch: 428/500\n",
      "\n",
      "Epoch: 429/500\n",
      "\n",
      "Epoch: 430/500\n",
      "\n",
      "Epoch: 431/500\n",
      "\n",
      "Epoch: 432/500\n",
      "\n",
      "Epoch: 433/500\n",
      "\n",
      "Epoch: 434/500\n",
      "\n",
      "Epoch: 435/500\n",
      "\n",
      "Epoch: 436/500\n",
      "\n",
      "Epoch: 437/500\n",
      "\n",
      "Epoch: 438/500\n",
      "\n",
      "Epoch: 439/500\n",
      "\n",
      "Epoch: 440/500\n",
      "\n",
      "Epoch: 441/500\n",
      "\n",
      "Epoch: 442/500\n",
      "\n",
      "Epoch: 443/500\n",
      "\n",
      "Epoch: 444/500\n",
      "\n",
      "Epoch: 445/500\n",
      "\n",
      "Epoch: 446/500\n",
      "\n",
      "Epoch: 447/500\n",
      "\n",
      "Epoch: 448/500\n",
      "\n",
      "Epoch: 449/500\n",
      "\n",
      "Epoch: 450/500\n",
      "\n",
      "Epoch: 451/500\n",
      "\n",
      "Epoch: 452/500\n",
      "\n",
      "Epoch: 453/500\n",
      "\n",
      "Epoch: 454/500\n",
      "\n",
      "Epoch: 455/500\n",
      "\n",
      "Epoch: 456/500\n",
      "\n",
      "Epoch: 457/500\n",
      "\n",
      "Epoch: 458/500\n",
      "\n",
      "Epoch: 459/500\n",
      "\n",
      "Epoch: 460/500\n",
      "\n",
      "Epoch: 461/500\n",
      "\n",
      "Epoch: 462/500\n",
      "\n",
      "Epoch: 463/500\n",
      "\n",
      "Epoch: 464/500\n",
      "\n",
      "Epoch: 465/500\n",
      "\n",
      "Epoch: 466/500\n",
      "\n",
      "Epoch: 467/500\n",
      "\n",
      "Epoch: 468/500\n",
      "\n",
      "Epoch: 469/500\n",
      "\n",
      "Epoch: 470/500\n",
      "\n",
      "Epoch: 471/500\n",
      "\n",
      "Epoch: 472/500\n",
      "\n",
      "Epoch: 473/500\n",
      "\n",
      "Epoch: 474/500\n",
      "\n",
      "Epoch: 475/500\n",
      "\n",
      "Epoch: 476/500\n",
      "\n",
      "Epoch: 477/500\n",
      "\n",
      "Epoch: 478/500\n",
      "\n",
      "Epoch: 479/500\n",
      "\n",
      "Epoch: 480/500\n",
      "\n",
      "Epoch: 481/500\n",
      "\n",
      "Epoch: 482/500\n",
      "\n",
      "Epoch: 483/500\n",
      "\n",
      "Epoch: 484/500\n",
      "\n",
      "Epoch: 485/500\n",
      "\n",
      "Epoch: 486/500\n",
      "\n",
      "Epoch: 487/500\n",
      "\n",
      "Epoch: 488/500\n",
      "\n",
      "Epoch: 489/500\n",
      "\n",
      "Epoch: 490/500\n",
      "\n",
      "Epoch: 491/500\n",
      "\n",
      "Epoch: 492/500\n",
      "\n",
      "Epoch: 493/500\n",
      "\n",
      "Epoch: 494/500\n",
      "\n",
      "Epoch: 495/500\n",
      "\n",
      "Epoch: 496/500\n",
      "\n",
      "Epoch: 497/500\n",
      "\n",
      "Epoch: 498/500\n",
      "\n",
      "Epoch: 499/500\n",
      "\n",
      "Epoch: 500/500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Fully_connected_Neural_Network at 0x7f05ff238860>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "x_axis = np.arange(nn.cost_iter.shape[0])\n",
    "plt.plot(x_axis, nn.cost_iter)\n",
    "# # # Name the plot\n",
    "# # # TODO Implement\n",
    "plt.title(\"Training Error after each Iteration\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "x_axis = np.arange(len(nn.cost_))\n",
    "plt.plot(x_axis, nn.cost_)\n",
    "# # # Name the plot\n",
    "# # # TODO Implement\n",
    "plt.title(\"training error for every epoch\")\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 10.00%\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "y_pred = nn.inference(X_trainval)\n",
    "\n",
    "\n",
    "count = 0\n",
    "indx = 0\n",
    "for i in y_pred:\n",
    "   # print(y_train[indx], np.max(i), i)\n",
    "    if Y_trainval[indx] == np.argmax(i) :\n",
    "        count = count+1\n",
    "    indx = indx+1\n",
    "\n",
    "acc=count/indx\n",
    "\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "y_pred1 = nn.inference(X_test)\n",
    "\n",
    "count1 = 0\n",
    "indx1 = 0\n",
    "for i in y_pred1:\n",
    "   # print(i)\n",
    "   # print(y_train[indx], np.max(i), i)\n",
    "    if Y_test[indx1] == np.argmax(i) :\n",
    "        count1 = count1+1\n",
    "    indx1 = indx1+1\n",
    "\n",
    "acc1=count1/indx1\n",
    "print('Test accuracy: %.2f%%' % (acc1 * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
